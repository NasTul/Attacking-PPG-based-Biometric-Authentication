{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPVovziU2glbVxsvxwV4Poe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uxoGaw51Nbz5"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import pdb\n","import pickle\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix\n","import pywt\n","\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Dense, RepeatVector, TimeDistributed, Flatten, LSTM, Bidirectional, Dropout, BatchNormalization, Conv2D, Activation, MaxPooling2D, Flatten, Conv1D, MaxPooling1D\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.losses import mse\n","from tensorflow.keras.optimizers import Adam, RMSprop, Adagrad\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import TensorBoard\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.initializers import glorot_normal\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from collections import Counter\n","import librosa\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import scipy.io\n","from scipy import stats\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.utils import resample\n","from sklearn.utils import class_weight\n","from sklearn.model_selection import KFold\n","import time\n","import os\n","import shutil\n","from tqdm import tqdm\n","mm = MinMaxScaler()"]},{"cell_type":"code","source":["import pickle\n","# load PPG data\n","file_open=open('xxx/UBFC_data/cycle_ppg_UBFC_T1_S1_S56.pl','rb')\n","user_data_x,user_data_y=pickle.load(file_open)\n","file_open.close()\n","\n","# load rPPG data\n","file_open=open('xxx/UBFC_data/cycle_rppg_UBFC_T1_S1_S56.pl','rb')\n","user_data_rppgx,user_data_rppgy=pickle.load(file_open)\n","file_open.close()\n","\n","y_list = list(Counter(user_data_rppgy).keys()) "],"metadata":{"id":"kJh7qZplNqFk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CNN-LSTM"],"metadata":{"id":"eTUvvYBaNqwX"}},{"cell_type":"code","source":["arr_result = []\n","\n","for sub_number in y_list:\n","  arr_sub_arr_result=[]\n","  test_ID = sub_number\n","  print(test_ID)\n","  arr_size = 60\n","  user_test_data_arr = []\n","  cycle_ppg_arr_x = []\n","  cycle_ppg_arr_y = []\n","  train_data_x = []\n","  test_data_x = []\n","  for index, data in enumerate(user_data_x) :\n","    if user_data_y[index]!=test_ID:\n","      continue\n","    resample = scipy.signal.resample(data,arr_size)\n","    resample_nor  =  mm.fit_transform(resample.reshape(-1,1)).flatten()\n","    user_test_data_arr.append(resample_nor)\n","  mean_arr = np.mean(user_test_data_arr, axis=0) \n","  for i in user_test_data_arr:\n","    if stats.pearsonr(i,mean_arr)[0]<0.9:\n","      continue\n","    else:\n","      cycle_ppg_arr_x.append(i)\n","      cycle_ppg_arr_y.append(test_ID)\n","      train_data_x.append(i)\n","\n","  for index, data in enumerate(user_data_x) :\n","\n","    if user_data_y[index]==test_ID:\n","      continue\n","    resample = scipy.signal.resample(data, arr_size)\n","    resample_nor  = mm.fit_transform(resample.reshape(-1,1)).flatten()\n","\n","    if index%10==0:\n","      cycle_ppg_arr_x.append(resample_nor)\n","      cycle_ppg_arr_y.append(user_data_y[index])\n","    else:\n","      test_data_x.append(resample_nor)\n","  \n","  \n","\n","  X = np.array(cycle_ppg_arr_x)\n","  Y = np.array(cycle_ppg_arr_y)\n","\n","  # print('test ID',test_ID)\n","  Y[np.where(Y!=test_ID)] = 0\n","\n","  arr_sub_arr_result.append(test_ID)\n","\n","  # number of class\n","  n_classes= 2\n","  # number of features\n","  n_features=X.shape[1]\n","\n","  enc = OneHotEncoder()\n","  # randomly shuffle data before training and testing\n","\n","  randIndx = np.arange(X.shape[0])\n","  np.random.seed(111)\n","  np.random.shuffle(randIndx)\n","\n","  trainSamples=np.floor(X.shape[0]*0.7).astype(int)\n","  testSamples=np.floor(X.shape[0]*0.3).astype(int) \n","\n","\n","  X_new = X[randIndx,:] \n","  \n","  Y_new =enc.fit_transform(Y[randIndx].reshape(-1,1)).toarray() \n","\n","\n","  # Assign training and testing set\n","  nChannels=1\n","\n","  train_data, train_target = np.reshape(X_new[:trainSamples,:],[trainSamples,n_features,nChannels]), Y_new[:trainSamples,:]\n","  try:\n","    test_data, test_target = np.reshape(X_new[trainSamples+1:,:],[testSamples,n_features,nChannels]), Y_new[trainSamples+1:,:]\n","  except:\n","    test_data, test_target = np.reshape(X_new[trainSamples:,:],[testSamples,n_features,nChannels]), Y_new[trainSamples:,:]\n","\n","\n","\n","  unique, counts = np.unique(train_target[:,1], return_counts=True)\n","  arr_sub_arr_result.append(counts[0])\n","  arr_sub_arr_result.append(counts[1])\n","\n","  unique, counts = np.unique(test_target[:,1], return_counts=True)\n","  arr_sub_arr_result.append(counts[0])\n","  arr_sub_arr_result.append(counts[1])\n","\n","  #first CNN\n","  model = Sequential()\n","  model.add(Conv1D(filters=25, kernel_size=6, padding='valid',strides=1, input_shape=[60,1]))\n","  model.add(Activation('relu'))\n","  model.add(BatchNormalization())\n","  model.add(MaxPooling1D(pool_size=4))\n","  model.add(Dropout(0.25))\n","\n","  #second CNN\n","  model.add(Conv1D(filters=40, kernel_size=10, padding='valid'))\n","  model.add(Activation('relu'))\n","  model.add(BatchNormalization())\n","  model.add(MaxPooling1D(pool_size=4))\n","  model.add(Dropout(0.25))\n","\n","  #first LSTM. note that we need to do a timedistributed flatten as a transition from CNN to LSTM\n","  model.add(TimeDistributed(Flatten()))\n","  model.add(Bidirectional(LSTM(units=32, return_sequences=True, dropout=0.25)))\n","  model.add(Flatten())\n","\n","  #activation layer\n","  model.add(Dense(2, activation='softmax'))\n","\n","  #compile model\n","  model.compile(\n","      loss='categorical_crossentropy',\n","      # loss='binary_crossentropy',\n","\n","      optimizer=Adam(),\n","      metrics=['accuracy'],\n","  )\n","\n","  #LSTM parameters\n","  BATCH_SIZE =20 #batch size\n","  EPOCHS = 50     #number of epochs\n","\n","\n","  # early_stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3)\n","\n","  fited_model = model.fit(\n","    train_data,\n","    train_target,\n","    batch_size=BATCH_SIZE,\n","    epochs=EPOCHS,\n","    verbose=1,\n","    # callbacks=[early_stop],\n","    shuffle=True,\n","    validation_data=(test_data,test_target),\n","  )\n","\n","  model.save(save_model_location+'/model_'+str(test_ID)+'.h5')\n","\n","\n","\n","  # Find Threshold\n","  thres = np.arange(0, 1.0001, 0.0001)\n","\n","  thres_matrix_test = np.zeros((len(test_target),len(thres)))\n","  total_matrix_test = np.zeros((len(thres),4)) #acc,far,frr,recall(order of column)\n","  test_logit_temp=model.predict(test_data)  \n","\n","  for j in range(len(thres)):\n","      for i in range(len(test_target)):\n","          if test_logit_temp[i][1] < thres[j]:\n","              thres_matrix_test[i,j] = 0\n","          else:\n","              thres_matrix_test[i,j] = 1\n","\n","  for i in range(len(thres)):\n","\n","      equal_logit = np.equal(thres_matrix_test[:,i], test_target[:,1]) ##Test\n","      unique2, True_pos_neg_test = np.unique(thres_matrix_test[np.where(equal_logit==True),i], return_counts=True)\n","      unique3, False_pos_neg_test = np.unique(thres_matrix_test[np.where(equal_logit==False),i], return_counts=True) \n","\n","      if np.shape(True_pos_neg_test)==(1,):\n","          if unique2[0] == 0:\n","              True_pos_neg_test = [True_pos_neg_test[0], 0]\n","          else:\n","              True_pos_neg_test = [0, True_pos_neg_test[0]]\n","\n","      if np.shape(False_pos_neg_test)==(1,):\n","          if unique3[0] == 0:\n","              False_pos_neg_test = [False_pos_neg_test[0], 0]\n","          else:\n","              False_pos_neg_test = [0, False_pos_neg_test[0]]\n","    \n","      if np.shape(True_pos_neg_test)==(0,):\n","          True_pos_neg_test = [0, 0]\n","\n","      if np.shape(False_pos_neg_test)==(0,):\n","          False_pos_neg_test = [0, 0]\n","  \n","      Recall_test= True_pos_neg_test[1]/(False_pos_neg_test[0]+True_pos_neg_test[1])\n","      Specific_test = True_pos_neg_test[0] /(True_pos_neg_test[0]+False_pos_neg_test[1])\n","\n","      ACC_test = (True_pos_neg_test[0]+True_pos_neg_test[1]) / (len(test_target))\n","      FAR_test = 1 - Specific_test\n","      FRR_test = 1 - Recall_test\n","\n","      total_matrix_test[i,:] = ACC_test, FAR_test, FRR_test, Recall_test\n","\n","  fig4 = plt.figure()\n","  ax = plt.subplot(111)\n","  ax.plot(thres, total_matrix_test[:,1], 'b', label='FAR')\n","  ax.plot(thres, total_matrix_test[:,2],'r', label='FRR')\n","  ax.legend()\n","  plt.title('FAR vs FRR (Test)')\n","  plt.xlabel('Threshold')\n","  plt.ylabel('Err Rate')\n","  plt.show()\n","\n","  EER_line = thres[::-1]\n","  fig5 = plt.figure()\n","  ax2 = plt.subplot(111)\n","  ax2.plot(total_matrix_test[:,1], total_matrix_test[:,3],'r',label='ROC')\n","  ax2.plot(thres, EER_line, 'b',label='EER Line')\n","  ax2.legend()\n","  plt.title('ROC Curve (Test)')\n","  plt.xlabel('FAR')\n","  plt.ylabel('TPR')\n","  plt.show()\n","\n","  EER_loc_test = np.argmin(abs(total_matrix_test[:,1] - total_matrix_test[:,2]))\n","  # print('EER threshold: ', thres[EER_loc_test])\n","  print('EER: {:.3f} '.format((total_matrix_test[EER_loc_test,1]+total_matrix_test[EER_loc_test,2])/2))\n","  print('FRR at EER: {:.3f} '.format(total_matrix_test[EER_loc_test,2]))\n","  print('FAR at EER: {:.3f} '.format(total_matrix_test[EER_loc_test,1]))\n","  print('Accuracy at EER: {:.3f} '.format(total_matrix_test[EER_loc_test,0]))\n","\n","\n","  file_open=open('xxx/UBFC_data/CNN_LSTM_model/thres_EER_loc_test_'+str(test_ID)+'.pl','wb')\n","  pickle.dump((thres,EER_loc_test),file_open)\n","  file_open.close()"],"metadata":{"id":"tc1F-qGSNqKR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"MNar1yPxSAPW"}},{"cell_type":"code","source":["# y_list = list(Counter(user_data_rppgy).keys()) \n","arr_result = []\n","\n","for sub_number in y_list:\n","  # if sub_number not in y_list1:\n","  #   continue\n","\n","  arr_sub_arr_result=[]\n","  test_ID = sub_number\n","  print(test_ID)\n","  arr_size = 60\n","  user_test_data_arr = []\n","  cycle_ppg_arr_x = []\n","  cycle_ppg_arr_y = []\n","  train_data_x = []\n","  test_data_x = []\n","  for index, data in enumerate(user_data_x) :\n","    if user_data_y[index]!=test_ID:\n","      continue\n","    resample = scipy.signal.resample(data,arr_size)\n","    resample_nor  =  mm.fit_transform(resample.reshape(-1,1)).flatten()\n","    user_test_data_arr.append(resample_nor)\n","  mean_arr = np.mean(user_test_data_arr, axis=0) \n","  for i in user_test_data_arr:\n","    if stats.pearsonr(i,mean_arr)[0]<0.9:\n","      continue\n","    else:\n","      cycle_ppg_arr_x.append(i)\n","      cycle_ppg_arr_y.append(test_ID)\n","      train_data_x.append(i)\n","\n","  for index, data in enumerate(user_data_x) :\n","\n","    if user_data_y[index]==test_ID:\n","      continue\n","    resample = scipy.signal.resample(data, arr_size)\n","    resample_nor  = mm.fit_transform(resample.reshape(-1,1)).flatten()\n","\n","    if index%10==0:\n","      cycle_ppg_arr_x.append(resample_nor)\n","      cycle_ppg_arr_y.append(user_data_y[index])\n","    else:\n","      test_data_x.append(resample_nor)\n","  \n","  \n","\n","  X = np.array(cycle_ppg_arr_x)\n","  Y = np.array(cycle_ppg_arr_y)\n","\n","  # print('test ID',test_ID)\n","  Y[np.where(Y!=test_ID)] = 0\n","\n","  arr_sub_arr_result.append(test_ID)\n","\n","  # number of class\n","  n_classes= 2\n","  # number of features\n","  n_features=X.shape[1]\n","\n","  enc = OneHotEncoder()\n","  # randomly shuffle data before training and testing\n","\n","  randIndx = np.arange(X.shape[0])\n","  np.random.seed(111)\n","  np.random.shuffle(randIndx)\n","\n","  trainSamples=np.floor(X.shape[0]*0.7).astype(int)\n","  testSamples=np.floor(X.shape[0]*0.3).astype(int) \n","\n","\n","  X_new = X[randIndx,:] \n","  \n","  Y_new =enc.fit_transform(Y[randIndx].reshape(-1,1)).toarray() \n","\n","\n","  # Assign training and testing set\n","  nChannels=1\n","\n","  train_data, train_target = np.reshape(X_new[:trainSamples,:],[trainSamples,n_features,nChannels]), Y_new[:trainSamples,:]\n","  try:\n","    test_data, test_target = np.reshape(X_new[trainSamples+1:,:],[testSamples,n_features,nChannels]), Y_new[trainSamples+1:,:]\n","  except:\n","    test_data, test_target = np.reshape(X_new[trainSamples:,:],[testSamples,n_features,nChannels]), Y_new[trainSamples:,:]\n","\n","\n","  # print(\"train:\",train_data.shape,train_target.shape)\n","  # print(\"test:\",test_data.shape,test_target.shape)\n","\n","  # Check # of 1 and 0 in training and testing set\n","\n","  unique, counts = np.unique(train_target[:,1], return_counts=True)\n","  # print('Train set dictionary: ',dict(zip(unique, counts)))\n","  arr_sub_arr_result.append(counts[0])\n","  arr_sub_arr_result.append(counts[1])\n","\n","  unique, counts = np.unique(test_target[:,1], return_counts=True)\n","  # print('Valid set dictionary: ',dict(zip(unique, counts)))\n","  arr_sub_arr_result.append(counts[0])\n","  arr_sub_arr_result.append(counts[1])\n","\n","  try:\n","    model = load_model(save_model_location+'/model_'+str(test_ID)+'.h5')\n","  except:\n","    continue\n","  thres = np.arange(0, 1.0001, 0.0001)\n","\n","  thres_matrix_test = np.zeros((len(test_target),len(thres)))\n","  total_matrix_test = np.zeros((len(thres),4)) #acc,far,frr,recall(order of column)\n","  test_logit_temp=model.predict(test_data)  \n","\n","  for j in range(len(thres)):\n","      for i in range(len(test_target)):\n","          if test_logit_temp[i][1] < thres[j]:\n","              thres_matrix_test[i,j] = 0\n","          else:\n","              thres_matrix_test[i,j] = 1\n","\n","  for i in range(len(thres)):\n","\n","      equal_logit = np.equal(thres_matrix_test[:,i], test_target[:,1]) ##Test\n","      unique2, True_pos_neg_test = np.unique(thres_matrix_test[np.where(equal_logit==True),i], return_counts=True)\n","      unique3, False_pos_neg_test = np.unique(thres_matrix_test[np.where(equal_logit==False),i], return_counts=True) \n","\n","      if np.shape(True_pos_neg_test)==(1,):\n","          if unique2[0] == 0:\n","              True_pos_neg_test = [True_pos_neg_test[0], 0]\n","          else:\n","              True_pos_neg_test = [0, True_pos_neg_test[0]]\n","\n","      if np.shape(False_pos_neg_test)==(1,):\n","          if unique3[0] == 0:\n","              False_pos_neg_test = [False_pos_neg_test[0], 0]\n","          else:\n","              False_pos_neg_test = [0, False_pos_neg_test[0]]\n","    \n","      if np.shape(True_pos_neg_test)==(0,):\n","          True_pos_neg_test = [0, 0]\n","          \n","      if np.shape(False_pos_neg_test)==(0,):\n","          False_pos_neg_test = [0, 0]\n","  \n","      Recall_test= True_pos_neg_test[1]/(False_pos_neg_test[0]+True_pos_neg_test[1])\n","      Specific_test = True_pos_neg_test[0] /(True_pos_neg_test[0]+False_pos_neg_test[1])\n","\n","      ACC_test = (True_pos_neg_test[0]+True_pos_neg_test[1]) / (len(test_target))\n","      FAR_test = 1 - Specific_test\n","      FRR_test = 1 - Recall_test\n","\n","      total_matrix_test[i,:] = ACC_test, FAR_test, FRR_test, Recall_test\n","\n","\n","  EER_loc_test = np.argmin(abs(total_matrix_test[:,1] - total_matrix_test[:,2]))\n","\n","  arr_sub_arr_result.append((total_matrix_test[EER_loc_test,1]+total_matrix_test[EER_loc_test,2])/2)\n","  arr_sub_arr_result.append(total_matrix_test[EER_loc_test,2])\n","  arr_sub_arr_result.append(total_matrix_test[EER_loc_test,1])\n","  arr_sub_arr_result.append(total_matrix_test[EER_loc_test,0])\n","\n","  test_data_x_pre =np.expand_dims(test_data_x,2) \n","\n","  result = model.predict(test_data_x_pre)\n","  final_result = result[:,1]\n","  final_result[np.where(final_result>thres[EER_loc_test])] = 1\n","  final_result[np.where(final_result<thres[EER_loc_test])] = 0\n","\n","  true_y = np.zeros(len(final_result))\n","  # print(\"Test data number:\",len(true_y))\n","  # print(\"FAR:\",(final_result[final_result == 1 ].size)/len(final_result))\n","  \n","  arr_sub_arr_result.append(len(true_y))\n","  arr_sub_arr_result.append((final_result[final_result == 1 ].size)/len(final_result))\n","\n","  try:\n","    # replace WGAN GP GMM result here #################################################\n","    file_open=open('wgan_rppg_result_'+str(test_ID)+'.pl','rb')\n","  except:\n","    print('except')\n","    continue\n","  b, preds_b=pickle.load(file_open)\n","  file_open.close()\n","\n","\n","  result = model.predict(np.expand_dims(b,2))\n","  final_result = result[:,1]\n","  final_result[np.where(final_result>thres[EER_loc_test])] = 1\n","  final_result[np.where(final_result<thres[EER_loc_test])] = 0\n","  true_y = np.zeros(len(final_result))\n","  # print(\"rPPG data number:\",len(true_y))\n","  # print(\"rPPG FAR:\",(final_result[final_result == 1 ].size)/len(final_result))\n","  arr_sub_arr_result.append(len(true_y))\n","  arr_sub_arr_result.append((final_result[final_result == 1 ].size)/len(final_result))\n","\n","  preds_b = np.squeeze(preds_b)\n","  result = model.predict(np.expand_dims(preds_b,2))\n","  final_result = result[:,1]\n","  final_result[np.where(final_result>thres[EER_loc_test])] = 1\n","  final_result[np.where(final_result<thres[EER_loc_test])] = 0\n","  true_y = np.zeros(len(final_result))\n","  (final_result[final_result == 1 ].size)/len(final_result)\n","  arr_sub_arr_result.append((final_result[final_result == 1 ].size)/len(final_result))\n","\n","\n","  sub_number=2\n","  max_pre = arr_sub_arr_result[-2]\n","  max_number = 0\n","  while sub_number < len(b):\n","\n","    mean_add = []\n","    sub_data = []\n","    for index, i in  enumerate(b):\n","      if index%sub_number==0 and index!=0:\n","        mean_add.append(np.mean(sub_data, axis=0) ) \n","        sub_data=[]\n","      else:\n","        resample = scipy.signal.resample(i,arr_size)\n","        resample_nor  =  mm.fit_transform(resample.reshape(-1,1)).flatten()\n","        sub_data.append(resample_nor)\n","    sub_number+=1\n","    mean_add = np.expand_dims(mean_add,2)\n","    result = model.predict(mean_add)\n","    final_result = result[:,1]\n","    final_result[np.where(final_result>thres[EER_loc_test])] = 1\n","    final_result[np.where(final_result<thres[EER_loc_test])] = 0\n","    true_y = np.zeros(len(final_result))\n","    far_result = (final_result[final_result == 1 ].size)/len(final_result)\n","    if far_result > max_pre:\n","      max_pre = far_result\n","      max_number = sub_number-1\n","    if max_pre==1:\n","      break\n","  # print(max_pre)\n","  # print(max_number)\n","\n","  arr_sub_arr_result.append(max_pre)\n","  arr_sub_arr_result.append(max_number)\n","\n","\n","  sub_number=2\n","  max_pre = arr_sub_arr_result[-3]\n","  max_number = 0\n","  last_len_data = 0\n","  while sub_number < len(preds_b):\n","\n","    mean_add = []\n","    sub_data = []\n","    for index, i in  enumerate(preds_b):\n","      if index%sub_number==0 and index!=0:\n","        mean_add.append(np.mean(sub_data, axis=0) ) \n","        sub_data=[]\n","      else:\n","        resample = scipy.signal.resample(i,arr_size)\n","        resample_nor  =  mm.fit_transform(resample.reshape(-1,1)).flatten()\n","        sub_data.append(resample_nor)\n","    if last_len_data == len(mean_add):\n","      sub_number+=1\n","      last_len_data=len(mean_add)\n","      continue\n","    last_len_data=len(mean_add)\n","    mean_add = np.expand_dims(mean_add,2)\n","    result = model.predict(mean_add)\n","    final_result = result[:,1]\n","    final_result[np.where(final_result>thres[EER_loc_test])] = 1\n","    final_result[np.where(final_result<thres[EER_loc_test])] = 0\n","    true_y = np.zeros(len(final_result))\n","    far_result = (final_result[final_result == 1 ].size)/len(final_result)\n","    if far_result > max_pre:\n","      max_pre = far_result\n","      max_number = sub_number-1\n","    if max_pre==1:\n","      break\n","\n","  arr_sub_arr_result.append(max_pre)\n","  arr_sub_arr_result.append(max_number)\n","  arr_result.append(arr_sub_arr_result)\n","\n"],"metadata":{"id":"hg2-_DfqSFP-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print result\n","import pandas as pd\n","df2 = pd.DataFrame(arr_result,\n","                   columns=['Test ID', 'Train0', 'Train1','Test0', 'Test1','EER','FRR','FAR', 'ACC','Test data',' Test FAR','rPPG number','rPPG FAR','WGAN FAR','rPPG MAX FAR','rPPG MAX num','WGAN MAX FAR', 'WGAN MAX num'])\n","\n","print('acc:',df2['ACC'].mean())\n","print('FAR:',df2['FAR'].mean())\n","print('rPPG FAR:',df2['rPPG FAR'].mean())\n","print('WGAN FAR:',df2['WGAN FAR'].mean())\n","print('rPPG MAX FAR:',df2['rPPG MAX FAR'].mean())\n","print('WGAN MAX FAR:',df2['WGAN MAX FAR'].mean())\n","\n","\n"],"metadata":{"id":"1CqyZ09sSFjG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"t0_6WQFgSFk3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1MsHwWxbSFmx"},"execution_count":null,"outputs":[]}]}