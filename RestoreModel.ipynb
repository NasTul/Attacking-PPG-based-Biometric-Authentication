{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["kq1X1Vr8C_wd","UjXvXXz-Dc12","9v-EIe8DHxt1"],"authorship_tag":"ABX9TyPIVrdfpdxbMiOFtREIkH1/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"K1QBVc3UCldN"},"outputs":[],"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","# TensorFlow and tf.keras\n","import tensorflow as tf\n","\n","import os\n","from tensorflow.keras import layers\n","\n","from tensorflow import keras\n","\n","# Helper libraries\n","import imageio\n","import numpy as np\n","import time\n","import matplotlib.pyplot as plt\n","from IPython import display\n","import PIL\n","import glob\n","from scipy import signal\n","import os \n","\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import scipy.io\n","from scipy import stats\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.utils import resample\n","from sklearn.utils import class_weight\n","from sklearn.model_selection import KFold\n","import time\n","import os\n","from scipy.fftpack import fft,ifft\n","\n","import sys\n","import json\n","import shutil\n","import argparse\n","import matplotlib\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","\n","from sklearn import svm\n","from scipy.signal import find_peaks\n","from scipy.signal import argrelmin,argrelmax\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","mm = MinMaxScaler()\n","\n","from collections import Counter\n","import pickle"]},{"cell_type":"code","source":["import pickle\n","# load PPG data\n","file_open=open('xxx/UBFC_data/cycle_ppg_UBFC_T1_S1_S56.pl','rb')\n","user_data_x,user_data_y=pickle.load(file_open)\n","file_open.close()\n","\n","# load rPPG data\n","file_open=open('xxx/UBFC_data/cycle_rppg_UBFC_T1_S1_S56.pl','rb')\n","user_data_rppgx,user_data_rppgy=pickle.load(file_open)\n","file_open.close()\n","\n","y_list = list(Counter(user_data_rppgy).keys()) "],"metadata":{"id":"hcMGneUMDA3g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## WGAN"],"metadata":{"id":"kq1X1Vr8C_wd"}},{"cell_type":"code","source":["checkpoint_dir_folder = 'xxx/UBFC_data/WGAN_Check_Point'"],"metadata":{"id":"DnYpe5TFDCEw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train_x == rppg\n","#except_X == ppg\n","#train_y == label\n","\n","\n","arr_size = 60\n","train_x =[]\n","except_x = []\n","train_y = []\n","keys_list = Counter(user_data_rppgy).keys()\n","for number_ in range(1,57):\n","  if number_ in keys_list:\n","    test_ID = number_\n","    user_test_data_arr = []\n","    cycle_rppg_arr_x = []\n","    cycle_rppg_arr_y = [] \n","    for index, data in enumerate(user_data_rppgx) :\n","      if user_data_rppgy[index]!=test_ID:\n","        continue\n","      resample = scipy.signal.resample(data,arr_size)\n","      resample_nor  =  mm.fit_transform(resample.reshape(-1,1)).flatten()\n","      user_test_data_arr.append(resample_nor)\n","    mean_rppg_arr = np.mean(user_test_data_arr, axis=0) \n","    for i in user_test_data_arr:\n","      if stats.pearsonr(i,mean_rppg_arr)[0]<0.9:\n","        continue\n","      else:\n","        cycle_rppg_arr_x.append(i)\n","        cycle_rppg_arr_y.append(test_ID)\n","\n","\n","    user_test_data_arr = []\n","    cycle_ppg_arr_x = []\n","    cycle_ppg_arr_y = [] \n","    for index, data in enumerate(user_data_x) :\n","      if user_data_y[index]!=test_ID:\n","        continue\n","      resample = scipy.signal.resample(data,arr_size)\n","      resample_nor  =  mm.fit_transform(resample.reshape(-1,1)).flatten()\n","      user_test_data_arr.append(resample_nor)\n","    mean_arr = np.mean(user_test_data_arr, axis=0) \n","    for i in user_test_data_arr:\n","      if stats.pearsonr(i,mean_arr)[0]<0.9:\n","        continue\n","      else:\n","        cycle_ppg_arr_x.append(i)\n","        cycle_ppg_arr_y.append(test_ID)\n","\n","    mean_sub_data = []\n","    sub_data = []\n","    mean_arr = np.mean(cycle_ppg_arr_x, axis=0) \n","\n","\n","    for index, i in enumerate(cycle_rppg_arr_x) :\n","\n","      train_x.append(i)\n","      except_x.append(mean_arr)\n","      train_y.append(test_ID)"],"metadata":{"id":"TEb2e26ODCGq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yzdCNuspDVni"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train WGAN"],"metadata":{"id":"UjXvXXz-Dc12"}},{"cell_type":"code","source":["for round_num in range(1,57):\n","    print(round_num)\n","\n","    test_ID=round_num\n","    train_x_id =[]\n","    except_x_id = []\n","    train_y_id = []\n","    rppg_train_x_test_id=[]\n","    for index, i in enumerate(train_x) :\n","      if train_y[index]==test_ID:\n","        rppg_train_x_test_id.append(i)\n","        continue\n","      train_x_id.append(i)\n","      except_x_id.append(except_x[index])\n","      train_y_id.append(train_y[index])\n","\n","    train_x_id=np.array(train_x_id)\n","    train_x_id = np.expand_dims(train_x_id,2)\n","    train_x_id.shape\n","\n","\n","    except_x_id=np.array(except_x_id)\n","    except_x_id = np.expand_dims(np.array(except_x_id),2)\n","    except_x_id.shape\n","\n","\n","    train_x_id = tf.convert_to_tensor(train_x_id,dtype=tf.float32)\n","\n","    except_x_id = tf.convert_to_tensor(except_x_id,dtype=tf.float32)\n","\n","    BATCH_SIZE = 100\n","    inputLength = 60\n","\n","    train_dataset = tf.data.Dataset.from_tensor_slices(train_x_id).batch(BATCH_SIZE)\n","    except_dataset = tf.data.Dataset.from_tensor_slices(except_x_id).batch(BATCH_SIZE)\n","\n","\n","    InputLength = 60\n","    Batch = 100\n","    \"\"\"######Generator\"\"\"\n","\n","    class Generator(tf.keras.Model):\n","      def __init__(self, InputLength=InputLength, Batch=Batch):\n","        super(Generator, self).__init__()\n","        self.start = layers.Dense(InputLength,input_shape=(InputLength,1),dtype='float32')\n","        self.active = layers.LeakyReLU(alpha=0.2)\n","        self.cov = layers.Conv1D(filters=32,kernel_size = 3, padding = 'causal')\n","        self.out = layers.Conv1D(filters = 1,kernel_size = 5, activation='tanh',padding = 'causal')\n","        self.dense = layers.Dense(1,dtype='float32')\n","        self.inference_net = tf.keras.Sequential(\n","          [\n","          self.active,\n","          self.cov,\n","          self.active,\n","          self.cov,\n","          self.active,\n","          self.cov,\n","          self.active,\n","          self.out\n","\n","          ]\n","        )\n","      def call(self, inputs):\n","        x = self.start(inputs)\n","        x = self.inference_net(x)\n","        return x\n","    generator = Generator()\n","\n","    \"\"\"######Discriminator\"\"\"\n","\n","    class Discriminator(tf.keras.Model):\n","      def __init__(self, InputLength=InputLength, Batch=Batch):\n","        super(Discriminator, self).__init__()\n","        self.start = layers.Conv1D(filters = 32,kernel_size = 3, padding = 'causal',input_shape=(InputLength,1))\n","        self.active = layers.LeakyReLU(alpha=0.2)\n","        self.cov = layers.Conv1D(filters=32, kernel_size = 3, padding = 'causal')\n","        self.pool = layers.MaxPooling1D(pool_size=2)\n","        self.flaten = layers.Flatten(input_shape=(InputLength,1))\n","        self.dense = layers.Dense(64,dtype='float32')\n","        self.drop = layers.Dropout(0.4)\n","        self.out = layers.Dense(1, activation='tanh',dtype='float32')\n","        self.inference_net = tf.keras.Sequential(\n","          [\n","          self.start,\n","          self.active,\n","          self.pool,\n","          self.cov,\n","          self.active,\n","          self.pool,\n","          self.flaten,\n","          self.dense,\n","          self.drop,\n","          self.active,\n","          self.out,\n","          ]\n","        )\n","      def call(self, inputs):\n","        x = self.inference_net(inputs)\n","        return x\n","    discriminator = Discriminator()\n","\n","\n","    # Define mix functions\n","    def mixSignal(real,fake):\n","      seed = np.random.randint(0,InputLength/2,1)[0]\n","      mix = tf.concat((real[:,seed:int(InputLength/2+seed),:],fake[:,seed:int(InputLength/2+seed),:]),1)\n","      return mix\n","    def wasserstein_loss(y_true, y_pred):\n","      return tf.reduce_mean(y_true * y_pred)\n","    def gradient_penalty_loss(mix):\n","\n","      \"\"\"\n","      Computes gradient penalty based on prediction and weighted real / fake samples\n","      \"\"\"\n","      with tf.GradientTape() as g:\n","        g.watch(mix)\n","        y = discriminator(mix)\n","      gradients = g.gradient(y, mix) \n","      # compute the euclidean norm by squaring ...\n","      gradients_sqr = tf.math.pow(gradients,2*tf.ones_like(gradients))\n","      #   ... summing over the rows ...\n","      gradients_sqr_sum = tf.reduce_sum(gradients_sqr,axis=np.arange(1, len(gradients_sqr.shape)))\n","      #   ... and sqrt\n","      gradient_l2_norm = tf.math.sqrt(gradients_sqr_sum)  \n","      # compute lambda * (1 - ||grad||)^2 still for each single sample\n","      gradient_penalty = tf.math.pow((1 - gradient_l2_norm),2*tf.ones_like(gradient_l2_norm))\n","      # return the mean as loss over all the batch samples\n","      return tf.reduce_mean(gradient_penalty)\n","\n","\n","    def discriminator_loss(real_output, fake_output, mix):\n","        real_loss = wasserstein_loss(-0.5*tf.ones_like(real_output), real_output)\n","        fake_loss = wasserstein_loss(0.5*tf.ones_like(fake_output), fake_output)\n","        mix_loss = gradient_penalty_loss(mix)\n","        total_loss = real_loss+fake_loss+10*mix_loss\n","        return total_loss\n","\n","    def generator_loss(fake_output,mix_output):\n","        mix_loss = wasserstein_loss(-0.5*tf.ones_like(fake_output), fake_output)\n","        fake_loss = wasserstein_loss(-0.5*tf.ones_like(fake_output), fake_output)\n","        return fake_loss+mix_loss\n","\n","    generator_optimizer = tf.keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n","    discriminator_optimizer = tf.keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n","\n","    checkpoint_dir = 'xxx/UBFC_data/WAGN_training_checkpoints/training_checkpoints_'+str(test_ID)\n","    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n","                                    discriminator_optimizer=discriminator_optimizer,\n","                                    generator=generator,\n","                                    discriminator=discriminator)\n","\n","\n","    \"\"\"######training\"\"\"\n","\n","\n","    @tf.function\n","    def train_step(train_x, except_x):\n","      for i in range(5):\n","        noise = tf.random.normal([Batch, InputLength,1])\n","        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","          generated_Data = generator(train_x, training=True)\n","          mix_Data = mixSignal(except_x,generated_Data)\n","\n","          real_output = discriminator(except_x, training=True)\n","          fake_output = discriminator(generated_Data, training=True)\n","          mix_output = discriminator(mix_Data, training=True)\n","\n","          gen_loss = generator_loss(fake_output,mix_output)\n","          disc_loss = discriminator_loss(real_output, fake_output,mix_Data)\n","\n","        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","        if i==4:\n","          generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","      return gen_loss,disc_loss\n","\n","\n","    def train(train_x_dataset,except_x_dataset, epochs):\n","      for epoch in range(epochs):\n","        start = time.time()\n","        num = 0\n","        for image_batch, image_batch_y in zip(train_x_dataset,except_x_dataset ):\n","          gen_loss,disc_loss = train_step(image_batch,image_batch_y)\n","          num = num+1\n","          if num%500==0:\n","            print ('generator loss {} discriminator loss {} sec'.format(gen_loss, disc_loss))\n","\n","            display.clear_output(wait=True)\n","\n","        if (epoch + 1) % 15 == 0:\n","          checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","          print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n","          print ('generator loss {} discriminator loss {} sec'.format(gen_loss, disc_loss))\n","\n","      display.clear_output(wait=True)\n","      \n","    def generate_and_save_images(model, epoch, test_input):\n","        predictions = model(test_input, training=False)\n","        for i in range(3):\n","            plt.plot(predictions[i].numpy()) \n","\n","        plt.show()\n","\n","\n","    train(train_dataset, except_dataset, 500)\n","\n","    # get pre result\n","    arr_size = 60\n","    user_test_data_arr = []\n","    cycle_ppg_arr_x = []\n","    cycle_ppg_arr_y = [] \n","    train_data_x = []\n","    test_data_x = []\n","    for index, data in enumerate(user_data_x) :\n","      if user_data_y[index]!=test_ID:\n","        continue\n","      resample = scipy.signal.resample(data,arr_size)\n","      resample_nor  =  mm.fit_transform(resample.reshape(-1,1)).flatten()\n","      user_test_data_arr.append(resample_nor)\n","    mean_arr = np.mean(user_test_data_arr, axis=0) \n","    for i in user_test_data_arr:\n","      if stats.pearsonr(i,mean_arr)[0]<0.9:\n","        continue\n","      else:\n","        cycle_ppg_arr_x.append(i)\n","        cycle_ppg_arr_y.append(test_ID)\n","        train_data_x.append(i)\n","\n","    for index, data in enumerate(user_data_x) :\n","\n","      if user_data_y[index]==test_ID:\n","        continue\n","      resample = scipy.signal.resample(data, arr_size)\n","      resample_nor  = mm.fit_transform(resample.reshape(-1,1)).flatten()\n","      cycle_ppg_arr_x.append(resample_nor)\n","      cycle_ppg_arr_y.append(user_data_y[index])\n","      test_data_x.append(resample_nor)\n","      \n","\n","    rppg_train_x_test_id = np.expand_dims(rppg_train_x_test_id,2)\n","    preds = generator(rppg_train_x_test_id,training=False)\n","\n","    b = np.squeeze(rppg_train_x_test_id)\n","    preds_b = np.squeeze(preds)\n","\n","\n","\n","    file_open=open('wgan_rppg_result_'+str(test_ID)+'.pl','wb')\n","    pickle.dump((b,preds_b),file_open)\n","    file_open.close()"],"metadata":{"id":"PMYNum0yDVpb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rdPDUhZ6DVrc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GP"],"metadata":{"id":"9v-EIe8DHxt1"}},{"cell_type":"code","source":["!pip install gpflow"],"metadata":{"id":"4h9eWQ2yDVth"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Tuple, Optional\n","import tempfile\n","import pathlib\n","\n","import datetime\n","import io\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","import tensorflow as tf\n","import gpflow\n","\n","from gpflow.config import default_float\n","from gpflow.ci_utils import ci_niter\n","from gpflow.utilities import to_default_float\n","\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"NdTJiN1eDVvY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for number_ in (y_list):\n","\n","    test_ID=number_\n","    train_x_id =[]\n","    except_x_id = []\n","    train_y_id = []\n","    rppg_train_x_test_id=[]\n","    for index, i in enumerate(train_x) :\n","      if train_y[index]==test_ID and test_ID!=10:\n","        rppg_train_x_test_id.append(i)\n","        continue\n","      train_x_id.append(i)\n","      except_x_id.append(except_x[index])\n","      train_y_id.append(train_y[index])\n","\n","\n","    train_x_id=np.array(train_x_id)\n","    train_x_id = np.expand_dims(train_x_id,2)\n","    train_x_id.shape\n","\n","\n","    except_x_id=np.array(except_x_id)\n","    except_x_id = np.expand_dims(np.array(except_x_id),2)\n","    except_x_id.shape\n","\n","\n","    train_x_id = tf.convert_to_tensor(train_x_id,dtype=tf.float64)\n","    except_x_id = tf.convert_to_tensor(except_x_id,dtype=tf.float64)\n","\n","    BATCH_SIZE = 32\n","    inputLength = 60\n","\n","    train_dataset = tf.data.Dataset.from_tensor_slices(train_x_id).batch(BATCH_SIZE)\n","    except_dataset = tf.data.Dataset.from_tensor_slices(except_x_id).batch(BATCH_SIZE)\n","\n","    # kernel = gpflow.kernels.SquaredExponential(variance=2.0)\n","    # kernel = gpflow.kernels.Linear()\n","    # kernel = gpflow.kernels.RationalQuadratic()\n","    kernel = gpflow.kernels.Matern32()\n","\n","\n","    likelihood = gpflow.likelihoods.Gaussian()\n","    inducing_variable = np.linspace(0, 60, 60).reshape(-1, 1)\n","\n","    model = gpflow.models.SVGP(\n","    kernel=kernel, likelihood=likelihood, inducing_variable=inducing_variable\n","    )\n","\n","    optimizer = tf.optimizers.Adam()\n","    def optimization_step(model: gpflow.models.SVGP, batch: Tuple[tf.Tensor, tf.Tensor]):\n","        with tf.GradientTape(watch_accessed_variables=False) as tape:\n","            tape.watch(model.trainable_variables)\n","            loss = model.training_loss(batch)\n","        grads = tape.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","        return loss\n","\n","\n","    def simple_training_loop(model: gpflow.models.SVGP, epochs: int = 1, logging_epoch_freq: int = 10):\n","        tf_optimization_step = tf.function(optimization_step)\n","\n","        batches = iter(train_dataset)\n","        for epoch in range(epochs):\n","            for _ in range(ci_niter(num_batches_per_epoch)):\n","                tf_optimization_step(model, next(batches))\n","\n","            epoch_id = epoch + 1\n","            if epoch_id % logging_epoch_freq == 0:\n","                tf.print(f\"Epoch {epoch_id}: ELBO (train) {model.elbo(data)}\")\n","\n","\n","    data = (train_x_id, except_x_id)\n","\n","    \n","    train_dataset = tf.data.Dataset.from_tensor_slices((train_x_id, except_x_id))\n","    test_dataset = tf.data.Dataset.from_tensor_slices((train_x_id, except_x_id))\n","\n","    batch_size = 32\n","    num_features = 60\n","    prefetch_size = tf.data.experimental.AUTOTUNE\n","    shuffle_buffer_size = len(train_x_id) // 2\n","    num_batches_per_epoch =  len(train_x_id) // batch_size\n","\n","    original_train_dataset = train_dataset\n","    train_dataset = (\n","        train_dataset.repeat()\n","        .prefetch(prefetch_size)\n","        .shuffle(buffer_size=shuffle_buffer_size)\n","        .batch(batch_size)\n","    )\n","\n","    simple_training_loop(model, epochs=10, logging_epoch_freq=5)\n","\n","\n","\n","    test_x_id=np.array(rppg_train_x_test_id)\n","    test_x_id = np.expand_dims(np.array(test_x_id),2)\n","    test_x_id.shape\n","    test_x_id = tf.convert_to_tensor(test_x_id,dtype=tf.float64)\n","\n","    model.predict_f_compiled = tf.function(\n","        model.predict_f, input_signature=[tf.TensorSpec(shape=[None, 60,1], dtype=tf.float64)]\n","    )\n","\n","    samples_input = tf.convert_to_tensor(test_x_id, dtype=default_float())\n","    original_result = model.predict_f_compiled(samples_input)\n","    test_result = original_result[0].numpy()\n","    plt.plot(original_result[0][0])\n","\n","    file_open=open('xxx/UBFC_data/wgan_rppg_result_T1_GP_'+str(test_ID)+'.pl','wb')\n","    pickle.dump((test_result,1),file_open)\n","    file_open.close()\n","\n","\n","    tf.saved_model.save(model, 'xxx/UBFC_data/GP_model/model'+str(test_ID))"],"metadata":{"id":"95zb5FvHDCIt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2b_SMD35DCKf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GMM"],"metadata":{"id":"nUwz7wjvKlc7"}},{"cell_type":"code","source":["!pip install nnmnkwii\n","import sklearn\n","from nnmnkwii.baseline.gmm import MLPG"],"metadata":{"id":"iWlKNhYNKki5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","for round_num in tqdm(y_list+[10]) :\n","    \n","    test_ID=round_num\n","    train_x_id =[]\n","    except_x_id = []\n","    train_y_id = []\n","    for index, i in enumerate(train_x) :\n","      if train_y[index]==test_ID:\n","        continue\n","      train_x_id.append(i)\n","      except_x_id.append(except_x[index])\n","      train_y_id.append(train_y[index])\n","\n","    train_x_id=np.array(train_x_id)\n","    train_x_id.shape\n","    except_x_id=np.array(except_x_id)\n","\n","    XY = np.concatenate((train_x_id, except_x_id), axis=-1)\n","    gmm = sklearn.mixture.GaussianMixture(n_components=3)\n","    _ = gmm.fit(XY)\n","    paramgen = MLPG(gmm,  windows=[(0,0, np.array([1.0]))], swap = True)\n","\n","    file_open=open('xxx/UBFC_data/wgan_rppg_result_GMM_'+str(number_)+'.pl','rb')\n","    rppg_train_x_test_id,_=pickle.load(file_open)\n","    file_open.close()\n","\n","\n","    try:\n","      generated = paramgen.transform(rppg_train_x_test_id)\n","    except:\n","      print('except',test_ID)\n","      continue\n","\n","    file_open=open('xxx/GMM_rppg_result_T1_'+str(test_ID)+'.pl','wb')\n","    pickle.dump((rppg_train_x_test_id,generated),file_open)\n","    file_open.close()\n","\n"],"metadata":{"id":"5EoMxg1CKkkj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nYSPgoWJMZxp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kYYeNTXiMZzi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"59RJNoT2MZ1l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GbvKNNtnMZ3n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oRNzgx1mMZ5U"},"execution_count":null,"outputs":[]}]}